# Data Cleaning Pipeline Documentation
## Airbnb Price Prediction Project

---

## ðŸŽ¯ Overview

This document describes the **function-based data cleaning pipeline** with **calendar aggregation** and **proper train/val/test splitting** to prevent data leakage.

---

## ðŸ“‹ Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Load Data                                           â”‚
â”‚  â€¢ listings_details.csv                                     â”‚
â”‚  â€¢ calendar.csv                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Calendar Aggregation                                â”‚
â”‚  â€¢ Clean calendar price & availability                      â”‚
â”‚  â€¢ Group by listing_id                                      â”‚
â”‚  â€¢ Create features:                                         â”‚
â”‚    - avg_calendar_price                                     â”‚
â”‚    - min_calendar_price                                     â”‚
â”‚    - max_calendar_price                                     â”‚
â”‚    - availability_rate                                      â”‚
â”‚    - calendar_days_count                                    â”‚
â”‚    - calendar_available_days                                â”‚
â”‚  â€¢ Merge with listings                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Train/Val/Test Split (60/20/20)                     â”‚
â”‚  âš ï¸ CRITICAL: Split BEFORE any transformations!             â”‚
â”‚  â€¢ Remove duplicates                                        â”‚
â”‚  â€¢ Remove missing/invalid prices                            â”‚
â”‚  â€¢ Split: 60% train, 20% val, 20% test                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: Define Preprocessing Function                       â”‚
â”‚  preprocess_data(X_train, X_val, X_test, y_train)          â”‚
â”‚                                                             â”‚
â”‚  1. Drop irrelevant columns                                â”‚
â”‚  2. Type conversion (price, percentage, boolean, numeric)  â”‚
â”‚  3. Logic error fixing (min > max nights)                  â”‚
â”‚  4. Drop high missing columns (>70%)                       â”‚
â”‚  5. Domain knowledge fills                                 â”‚
â”‚  6. Feature engineering (dates, text, amenities)           â”‚
â”‚  7. Cleanup (drop original text/date columns)              â”‚
â”‚  8. Categorical encoding (one-hot + target encoding)       â”‚
â”‚  9. Handle remaining missing values (TRAIN median)         â”‚
â”‚  10. Outlier treatment (TRAIN quantiles)                   â”‚
â”‚                                                             â”‚
â”‚  âœ… All transformations FIT on TRAIN, TRANSFORM all splits â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: Apply Preprocessing                                 â”‚
â”‚  X_train_clean, X_val_clean, X_test_clean, features, enc   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: Scaling (3 scalers available)                       â”‚
â”‚  â€¢ StandardScaler (default, best for most models)          â”‚
â”‚  â€¢ MinMaxScaler (neural networks)                          â”‚
â”‚  â€¢ RobustScaler (outlier-resistant)                        â”‚
â”‚                                                             â”‚
â”‚  All FIT on TRAIN, TRANSFORM all splits                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 7: Save Processed Data                                 â”‚
â”‚  â€¢ Unscaled CSV files                                       â”‚
â”‚  â€¢ Scaled NumPy arrays (3 versions)                        â”‚
â”‚  â€¢ Feature names                                            â”‚
â”‚  â€¢ Scalers & encoders (pickle)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”‘ Key Features

### âœ… **No Data Leakage**
- Data split **BEFORE** any transformations
- All statistics (mean, median, quantiles) computed from **training data only**
- Transformations applied consistently across train/val/test

### âœ… **Calendar Integration**
- Aggregates `calendar.csv` at the listing level
- Creates 6 new features capturing pricing and availability patterns
- Left join preserves all listings (handles missing calendar data)

### âœ… **Function-Based Architecture**
- Single `preprocess_data()` function for all transformations
- Reproducible and maintainable
- Easy to modify or extend
- Returns all processed datasets and metadata

### âœ… **Comprehensive Feature Engineering**
- **Date features**: Host tenure, review recency, cyclical encoding
- **Text features**: Length and word count for all text fields
- **Amenities**: Binary flags for key amenities
- **Calendar**: Pricing statistics and availability metrics

### âœ… **Proper Split Ratio**
- **60% Train**: Maximum data for model training
- **20% Validation**: Hyperparameter tuning and model selection
- **20% Test**: Final unbiased performance evaluation

---

## ðŸ“Š Data Flow

### Input Files
```
main_dataset/
â”œâ”€â”€ listings_details.csv    (20,030 listings Ã— 96 features)
â””â”€â”€ calendar.csv            (7,310,950 rows Ã— 4 features)
```

### Output Files
```
processed_data/
â”œâ”€â”€ train_unscaled.csv           # Unscaled training data
â”œâ”€â”€ val_unscaled.csv             # Unscaled validation data
â”œâ”€â”€ test_unscaled.csv            # Unscaled test data
â”‚
â”œâ”€â”€ X_train_standard.npy         # StandardScaler
â”œâ”€â”€ X_val_standard.npy
â”œâ”€â”€ X_test_standard.npy
â”‚
â”œâ”€â”€ X_train_minmax.npy           # MinMaxScaler
â”œâ”€â”€ X_val_minmax.npy
â”œâ”€â”€ X_test_minmax.npy
â”‚
â”œâ”€â”€ X_train_robust.npy           # RobustScaler
â”œâ”€â”€ X_val_robust.npy
â”œâ”€â”€ X_test_robust.npy
â”‚
â”œâ”€â”€ y_train.npy                  # Target values
â”œâ”€â”€ y_val.npy
â”œâ”€â”€ y_test.npy
â”‚
â”œâ”€â”€ feature_names.csv            # List of all features
â”œâ”€â”€ scaler_standard.pkl          # Fitted StandardScaler
â”œâ”€â”€ scaler_minmax.pkl            # Fitted MinMaxScaler
â”œâ”€â”€ scaler_robust.pkl            # Fitted RobustScaler
â””â”€â”€ encoders.pkl                 # All fitted encoders/stats
```

---

## ðŸ› ï¸ Usage

### Running the Pipeline
```python
# Simply run all cells in data_cleaning.ipynb
# The notebook will:
# 1. Load data
# 2. Aggregate calendar
# 3. Split data
# 4. Define preprocessing function
# 5. Apply preprocessing
# 6. Scale data
# 7. Save all outputs
```

### Loading Processed Data for Modeling

**Option 1: Unscaled Data (CSV)**
```python
import pandas as pd

train_df = pd.read_csv('processed_data/train_unscaled.csv')
val_df = pd.read_csv('processed_data/val_unscaled.csv')
test_df = pd.read_csv('processed_data/test_unscaled.csv')

X_train = train_df.drop(columns=['price'])
y_train = train_df['price']
```

**Option 2: Scaled Data (NumPy)**
```python
import numpy as np

# StandardScaler (recommended for most models)
X_train = np.load('processed_data/X_train_standard.npy')
X_val = np.load('processed_data/X_val_standard.npy')
X_test = np.load('processed_data/X_test_standard.npy')

y_train = np.load('processed_data/y_train.npy')
y_val = np.load('processed_data/y_val.npy')
y_test = np.load('processed_data/y_test.npy')
```

**Option 3: Load Scalers for New Data**
```python
import pickle

# Load fitted scaler
with open('processed_data/scaler_standard.pkl', 'rb') as f:
    scaler = pickle.load(f)

# Transform new data
X_new_scaled = scaler.transform(X_new)
```

---

## ðŸ“ˆ Feature Categories

### Calendar-Derived Features (6)
- `avg_calendar_price`: Mean price from calendar
- `min_calendar_price`: Minimum price from calendar
- `max_calendar_price`: Maximum price from calendar
- `availability_rate`: Proportion of available days
- `calendar_days_count`: Total days in calendar
- `calendar_available_days`: Number of available days

### Date Features (~10)
- Host tenure (days, years)
- Host since (year, month, day of week)
- Cyclical encoding (sin/cos for month)
- Days since first/last review
- Review period length

### Text Features (~20)
- Length and word count for:
  - name, summary, space, description
  - neighborhood_overview, notes, transit
  - access, interaction, house_rules

### Amenity Features (7)
- `amenities_count`: Total number of amenities
- Binary flags: wifi, kitchen, tv, parking, ac, heating

### Original Listing Features (~100+)
- Property details (bedrooms, bathrooms, accommodates)
- Location (latitude, longitude, neighborhood)
- Pricing (cleaning_fee, security_deposit)
- Reviews (scores, counts)
- Host information (response rate, superhost status)
- Availability (30, 60, 90, 365 days)

---

## âš ï¸ Important Notes

### Data Leakage Prevention
1. **Split first**: Always split before any transformations
2. **Fit on train**: All statistics computed from training data
3. **Transform all**: Apply learned transformations to val/test
4. **Never use val/test**: Don't peek at validation/test during preprocessing

### Missing Values Strategy
| Type | Strategy |
|------|----------|
| High missing (>70%) | Drop column |
| Security deposit | Fill with 0 |
| Review scores | Fill with 0 |
| Text fields | Fill with 'Unknown' |
| Categorical | Fill with training mode |
| Numeric | Fill with training median |

### Outlier Treatment
| Feature | Method |
|---------|--------|
| minimum_nights | Cap at 365 |
| maximum_nights | Cap at 730 |
| accommodates | Cap at 16 |
| cleaning_fee | Winsorize at 99th percentile (train) |
| security_deposit | Winsorize at 99th percentile (train) |

### Encoding Strategy
| Cardinality | Method |
|-------------|--------|
| < 10 unique values | One-hot encoding |
| â‰¥ 10 unique values | Target encoding (fit on train) |
| Boolean | Convert to 0/1 |

---

## ðŸ”„ Modifying the Pipeline

### Adding New Features
Add feature engineering code inside the `preprocess_data()` function in Step 6 (Feature Engineering section).

### Changing Split Ratio
Modify the `train_test_split` parameters in Step 3:
```python
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.4, random_state=42  # Adjust test_size
)
```

### Adding Another Scaler
Add a new scaler in Step 6:
```python
scaler_new = YourScaler()
X_train_new = scaler_new.fit_transform(X_train_clean)
X_val_new = scaler_new.transform(X_val_clean)
X_test_new = scaler_new.transform(X_test_clean)
```

---

## âœ… Quality Checks

The pipeline includes automatic verification:
- âœ… No NaN values after preprocessing
- âœ… No NaN values after scaling
- âœ… Consistent shapes across train/val/test
- âœ… Proper scaling statistics
- âœ… Feature alignment across splits

---

## ðŸ“ž Questions?

If you encounter issues or need modifications:
1. Check the cell outputs for error messages
2. Verify input file paths are correct
3. Ensure all required libraries are installed
4. Review the `encoders` dictionary for transformation details

---

**Last Updated**: November 2025  
**Pipeline Version**: 2.0 (Function-based with Calendar Integration)

