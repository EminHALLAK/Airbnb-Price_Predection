{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T19:43:08.253346Z",
     "start_time": "2025-11-13T19:43:08.249959Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "âœ“ All libraries imported successfully\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AIRBNB PRICE PREDICTION - DATA CLEANING PIPELINE\n",
    "================================================\n",
    "This notebook implements a clean data preprocessing pipeline with:\n",
    "1. Calendar aggregation with listings\n",
    "2. Train/Val/Test split (60/20/20)\n",
    "3. Preprocessing functions applied to each split\n",
    "4. No data leakage\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "import io\n",
    "\n",
    "# Set UTF-8 encoding for Windows\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"âœ“ All libraries imported successfully\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ddcb2a95f6ec65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T19:43:10.008222Z",
     "start_time": "2025-11-13T19:43:08.269568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: LOAD DATA\n",
      "================================================================================\n",
      "âœ“ Listings loaded: (20030, 96)\n",
      "âœ“ Calendar loaded: (7310950, 4)\n",
      "  Columns: ['listing_id', 'date', 'available', 'price']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 1: LOAD DATA\n",
    "=================\n",
    "Load listings and calendar datasets\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: LOAD DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load listings\n",
    "listings_df = pd.read_csv('main_dataset/listings_details.csv')\n",
    "print(f\"âœ“ Listings loaded: {listings_df.shape}\")\n",
    "\n",
    "# Load calendar\n",
    "calendar_df = pd.read_csv('main_dataset/calendar.csv')\n",
    "print(f\"âœ“ Calendar loaded: {calendar_df.shape}\")\n",
    "print(f\"  Columns: {calendar_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "740555dd777262b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T19:43:12.647215Z",
     "start_time": "2025-11-13T19:43:10.050669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: CALENDAR AGGREGATION\n",
      "================================================================================\n",
      "âœ“ Calendar data cleaned\n",
      "  Available values: {0: 6110879, 1: 1200071}\n",
      "âœ“ Calendar aggregated: (20030, 7)\n",
      "  Features created: ['listing_id', 'avg_calendar_price', 'min_calendar_price', 'max_calendar_price', 'availability_rate', 'calendar_days_count', 'calendar_available_days']\n",
      "âœ“ Merged with listings: (20030, 102)\n",
      "  Missing calendar data: 9344 listings\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 2: CALENDAR AGGREGATION\n",
    "=============================\n",
    "Aggregate calendar data by listing_id to create new features\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: CALENDAR AGGREGATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clean calendar price column\n",
    "def clean_calendar_price(price_str):\n",
    "    if pd.isna(price_str):\n",
    "        return np.nan\n",
    "    if isinstance(price_str, (int, float)):\n",
    "        return float(price_str)\n",
    "    return float(str(price_str).replace('$', '').replace(',', ''))\n",
    "\n",
    "calendar_df['price_clean'] = calendar_df['price'].apply(clean_calendar_price)\n",
    "\n",
    "# Convert available column\n",
    "calendar_df['is_available'] = calendar_df['available'].map({'t': 1, 'f': 0})\n",
    "\n",
    "# Convert date to datetime\n",
    "calendar_df['date'] = pd.to_datetime(calendar_df['date'], errors='coerce')\n",
    "\n",
    "print(f\"âœ“ Calendar data cleaned\")\n",
    "print(f\"  Available values: {calendar_df['is_available'].value_counts().to_dict()}\")\n",
    "\n",
    "# Aggregate calendar features by listing_id\n",
    "calendar_agg = (\n",
    "    calendar_df\n",
    "    .groupby('listing_id')\n",
    "    .agg(\n",
    "        avg_calendar_price=('price_clean', 'mean'),\n",
    "        min_calendar_price=('price_clean', 'min'),\n",
    "        max_calendar_price=('price_clean', 'max'),\n",
    "        availability_rate=('is_available', 'mean'),\n",
    "        calendar_days_count=('date', 'count'),\n",
    "        calendar_available_days=('is_available', 'sum')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Calendar aggregated: {calendar_agg.shape}\")\n",
    "print(f\"  Features created: {calendar_agg.columns.tolist()}\")\n",
    "\n",
    "# Merge with listings\n",
    "df = listings_df.merge(\n",
    "    calendar_agg,\n",
    "    left_on='id',\n",
    "    right_on='listing_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop duplicate listing_id column from merge\n",
    "if 'listing_id' in df.columns:\n",
    "    df = df.drop(columns=['listing_id'])\n",
    "\n",
    "print(f\"âœ“ Merged with listings: {df.shape}\")\n",
    "print(f\"  Missing calendar data: {df['avg_calendar_price'].isnull().sum()} listings\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "fb233129da2e1e4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T19:44:48.306639Z",
     "start_time": "2025-11-13T19:44:48.206903Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "STEP 3: TRAIN/VAL/TEST SPLIT (60/20/20)\n",
    "========================================\n",
    "Split BEFORE any transformations to prevent data leakage\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: TRAIN/VAL/TEST SPLIT (60/20/20)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Remove duplicates first\n",
    "df = df.drop_duplicates(subset=['id'], keep='first')\n",
    "print(f\"âœ“ Removed duplicates. Shape: {df.shape}\")\n",
    "\n",
    "# Convert price to numeric FIRST (it's loaded as string with $ signs)\n",
    "if 'price' in df.columns:\n",
    "    def clean_price(price_str):\n",
    "        if pd.isna(price_str):\n",
    "            return np.nan\n",
    "        if isinstance(price_str, (int, float)):\n",
    "            return float(price_str)\n",
    "        # Remove $ and commas, then convert to float\n",
    "        return float(str(price_str).replace('$', '').replace(',', ''))\n",
    "    \n",
    "    df['price'] = df['price'].apply(clean_price)\n",
    "    print(f\"âœ“ Converted price column to numeric\")\n",
    "\n",
    "# Remove rows with missing target variable (price)\n",
    "if 'price' in df.columns:\n",
    "    before_count = len(df)\n",
    "    df = df[df['price'].notna()]\n",
    "    removed = before_count - len(df)\n",
    "    if removed > 0:\n",
    "        print(f\"âœ“ Removed {removed} rows with missing price\")\n",
    "\n",
    "# Remove zero or negative prices\n",
    "if 'price' in df.columns:\n",
    "    before_count = len(df)\n",
    "    df = df[df['price'] > 0]\n",
    "    removed = before_count - len(df)\n",
    "    if removed > 0:\n",
    "        print(f\"âœ“ Removed {removed} rows with zero/negative price\")\n",
    "\n",
    "print(f\"Final shape before split: {df.shape}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']\n",
    "\n",
    "# Split: 60% train, 20% val, 20% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, shuffle=True\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Split completed:\")\n",
    "print(f\"  Train: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Val:   {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test:  {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Store train indices for reference\n",
    "train_ids = X_train['id'].values if 'id' in X_train.columns else None\n",
    "print(f\"\\nâœ“ Data split complete. Ready for preprocessing functions.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: TRAIN/VAL/TEST SPLIT (60/20/20)\n",
      "================================================================================\n",
      "âœ“ Removed duplicates. Shape: (20030, 102)\n",
      "âœ“ Converted price column to numeric\n",
      "âœ“ Removed 2 rows with zero/negative price\n",
      "Final shape before split: (20028, 102)\n",
      "\n",
      "âœ“ Split completed:\n",
      "  Train: 12,016 samples (60.0%)\n",
      "  Val:   4,006 samples (20.0%)\n",
      "  Test:  4,006 samples (20.0%)\n",
      "  Features: 101\n",
      "\n",
      "âœ“ Data split complete. Ready for preprocessing functions.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "2aba9163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T19:44:55.980533Z",
     "start_time": "2025-11-13T19:44:55.955166Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "STEP 4: DEFINE PREPROCESSING FUNCTIONS\n",
    "=======================================\n",
    "All preprocessing functions that will be applied to split data\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: DEFINE PREPROCESSING FUNCTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def preprocess_data(X_train, X_val, X_test, y_train, verbose=True):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline that fits on train and transforms all splits.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_val, X_test : pandas DataFrames\n",
    "        Feature matrices for each split\n",
    "    y_train : pandas Series\n",
    "        Target variable for training set (needed for target encoding)\n",
    "    verbose : bool\n",
    "        Whether to print progress\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (X_train_processed, X_val_processed, X_test_processed, feature_names, encoders)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make copies to avoid modifying originals\n",
    "    X_train = X_train.copy()\n",
    "    X_val = X_val.copy()\n",
    "    X_test = X_test.copy()\n",
    "    \n",
    "    encoders = {}  # Store fitted encoders/transformers\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"1. DROPPING IRRELEVANT COLUMNS\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Drop irrelevant columns\n",
    "    cols_to_drop = [\n",
    "        'listing_url', 'scrape_id', 'last_scraped', 'thumbnail_url', 'medium_url',\n",
    "        'picture_url', 'xl_picture_url', 'host_id', 'host_url', 'host_thumbnail_url',\n",
    "        'host_picture_url', 'license', 'jurisdiction_names', 'calendar_last_scraped',\n",
    "        'experiences_offered', 'neighbourhood_group_cleansed', 'id'\n",
    "    ]\n",
    "    \n",
    "    # Drop columns with 100% missing in TRAINING data only\n",
    "    missing_100_cols = X_train.columns[X_train.isnull().mean() == 1.0].tolist()\n",
    "    cols_to_drop.extend(missing_100_cols)\n",
    "    \n",
    "    cols_dropped = [c for c in cols_to_drop if c in X_train.columns]\n",
    "    X_train = X_train.drop(columns=cols_dropped, errors='ignore')\n",
    "    X_val = X_val.drop(columns=cols_dropped, errors='ignore')\n",
    "    X_test = X_test.drop(columns=cols_dropped, errors='ignore')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  âœ“ Dropped {len(cols_dropped)} columns\")\n",
    "        print(f\"  Shape: Train {X_train.shape}, Val {X_val.shape}, Test {X_test.shape}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. TYPE CONVERSION\n",
    "    # ========================================================================\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"2. TYPE CONVERSION\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    def clean_price(price_str):\n",
    "        if pd.isna(price_str):\n",
    "            return np.nan\n",
    "        if isinstance(price_str, (int, float)):\n",
    "            return float(price_str)\n",
    "        return float(str(price_str).replace('$', '').replace(',', ''))\n",
    "    \n",
    "    # Price columns\n",
    "    price_cols = ['weekly_price', 'monthly_price', 'security_deposit',\n",
    "                  'cleaning_fee', 'extra_people']\n",
    "    for col in price_cols:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = X_train[col].apply(clean_price)\n",
    "            X_val[col] = X_val[col].apply(clean_price)\n",
    "            X_test[col] = X_test[col].apply(clean_price)\n",
    "    \n",
    "    # Percentage columns\n",
    "    percentage_cols = ['host_response_rate', 'host_acceptance_rate']\n",
    "    for col in percentage_cols:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = X_train[col].apply(lambda x: float(str(x).replace('%', '')) / 100 if pd.notna(x) else np.nan)\n",
    "            X_val[col] = X_val[col].apply(lambda x: float(str(x).replace('%', '')) / 100 if pd.notna(x) else np.nan)\n",
    "            X_test[col] = X_test[col].apply(lambda x: float(str(x).replace('%', '')) / 100 if pd.notna(x) else np.nan)\n",
    "    \n",
    "    # Boolean columns\n",
    "    bool_cols = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified',\n",
    "                 'is_location_exact', 'has_availability', 'instant_bookable',\n",
    "                 'is_business_travel_ready', 'require_guest_profile_picture',\n",
    "                 'require_guest_phone_verification', 'requires_license']\n",
    "    for col in bool_cols:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = X_train[col].map({'t': True, 'f': False, True: True, False: False})\n",
    "            X_val[col] = X_val[col].map({'t': True, 'f': False, True: True, False: False})\n",
    "            X_test[col] = X_test[col].map({'t': True, 'f': False, True: True, False: False})\n",
    "    \n",
    "    # Numeric columns\n",
    "    numeric_cols = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'guests_included',\n",
    "                    'minimum_nights', 'maximum_nights', 'availability_30', 'availability_60',\n",
    "                    'availability_90', 'availability_365', 'number_of_reviews',\n",
    "                    'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "                    'review_scores_checkin', 'review_scores_communication', 'review_scores_location',\n",
    "                    'review_scores_value', 'calculated_host_listings_count', 'reviews_per_month',\n",
    "                    'host_listings_count', 'host_total_listings_count', 'square_feet',\n",
    "                    'latitude', 'longitude']\n",
    "    for col in numeric_cols:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "            X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "            X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
    "    \n",
    "    # Date columns\n",
    "    date_cols = ['host_since', 'first_review', 'last_review']\n",
    "    for col in date_cols:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = pd.to_datetime(X_train[col], errors='coerce')\n",
    "            X_val[col] = pd.to_datetime(X_val[col], errors='coerce')\n",
    "            X_test[col] = pd.to_datetime(X_test[col], errors='coerce')\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"  âœ“ Type conversion completed\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. LOGIC ERROR FIXING\n",
    "    # ========================================================================\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"3. LOGIC ERROR FIXING\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Fix min > max nights (set to NaN, will be imputed later)\n",
    "    if 'minimum_nights' in X_train.columns and 'maximum_nights' in X_train.columns:\n",
    "        mask_train = X_train['minimum_nights'] > X_train['maximum_nights']\n",
    "        mask_val = X_val['minimum_nights'] > X_val['maximum_nights']\n",
    "        mask_test = X_test['minimum_nights'] > X_test['maximum_nights']\n",
    "        \n",
    "        X_train.loc[mask_train, ['minimum_nights', 'maximum_nights']] = np.nan\n",
    "        X_val.loc[mask_val, ['minimum_nights', 'maximum_nights']] = np.nan\n",
    "        X_test.loc[mask_test, ['minimum_nights', 'maximum_nights']] = np.nan\n",
    "        \n",
    "        if verbose and mask_train.sum() > 0:\n",
    "            print(f\"  âœ“ Fixed min/max night errors: Train={mask_train.sum()}, Val={mask_val.sum()}, Test={mask_test.sum()}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 4. DROP HIGH MISSING COLUMNS (Based on TRAINING data)\n",
    "    # ========================================================================\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"4. DROP HIGH MISSING COLUMNS\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    missing_pct = (X_train.isnull().sum() / len(X_train) * 100)\n",
    "    cols_to_drop_missing = missing_pct[missing_pct > 70].index.tolist()\n",
    "    \n",
    "    if cols_to_drop_missing:\n",
    "        X_train = X_train.drop(columns=cols_to_drop_missing)\n",
    "        X_val = X_val.drop(columns=cols_to_drop_missing)\n",
    "        X_test = X_test.drop(columns=cols_to_drop_missing)\n",
    "        if verbose:\n",
    "            print(f\"  âœ“ Dropped {len(cols_to_drop_missing)} columns with >70% missing in training data\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 5. DOMAIN KNOWLEDGE FILLS (Safe before other imputation)\n",
    "    # ========================================================================\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"5. DOMAIN KNOWLEDGE FILLS\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Security deposit: 0 means no deposit\n",
    "    if 'security_deposit' in X_train.columns:\n",
    "        X_train['security_deposit'] = X_train['security_deposit'].fillna(0)\n",
    "        X_val['security_deposit'] = X_val['security_deposit'].fillna(0)\n",
    "        X_test['security_deposit'] = X_test['security_deposit'].fillna(0)\n",
    "        if verbose:\n",
    "            print(\"  âœ“ Filled security_deposit with 0\")\n",
    "    \n",
    "    # Host neighbourhood from listing neighbourhood\n",
    "    if 'host_neighbourhood' in X_train.columns and 'neighbourhood_cleansed' in X_train.columns:\n",
    "        X_train['host_neighbourhood'] = X_train['host_neighbourhood'].fillna(X_train['neighbourhood_cleansed'])\n",
    "        X_val['host_neighbourhood'] = X_val['host_neighbourhood'].fillna(X_val['neighbourhood_cleansed'])\n",
    "        X_test['host_neighbourhood'] = X_test['host_neighbourhood'].fillna(X_test['neighbourhood_cleansed'])\n",
    "        if verbose:\n",
    "            print(\"  âœ“ Filled host_neighbourhood from neighbourhood_cleansed\")\n",
    "    \n",
    "    # Review scores: 0 means no reviews\n",
    "    review_cols = ['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "                   'review_scores_checkin', 'review_scores_communication', 'review_scores_location',\n",
    "                   'review_scores_value', 'reviews_per_month']\n",
    "    for col in review_cols:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = X_train[col].fillna(0)\n",
    "            X_val[col] = X_val[col].fillna(0)\n",
    "            X_test[col] = X_test[col].fillna(0)\n",
    "    if verbose:\n",
    "        print(f\"  âœ“ Filled {len([c for c in review_cols if c in X_train.columns])} review columns with 0\")\n",
    "    \n",
    "    # Text columns: Unknown\n",
    "    text_cols = ['notes', 'transit', 'access', 'interaction', 'house_rules',\n",
    "                 'neighborhood_overview', 'host_about', 'host_response_time']\n",
    "    for col in text_cols:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = X_train[col].fillna('Unknown')\n",
    "            X_val[col] = X_val[col].fillna('Unknown')\n",
    "            X_test[col] = X_test[col].fillna('Unknown')\n",
    "    \n",
    "    # Categorical columns: Use TRAINING mode\n",
    "    cat_cols = ['name', 'summary', 'space', 'description', 'host_name', 'host_location',\n",
    "                'neighbourhood', 'city', 'state', 'zipcode', 'market']\n",
    "    for col in cat_cols:\n",
    "        if col in X_train.columns and X_train[col].isnull().sum() > 0:\n",
    "            mode_val = X_train[col].mode()[0] if len(X_train[col].mode()) > 0 else 'Unknown'\n",
    "            X_train[col] = X_train[col].fillna(mode_val)\n",
    "            X_val[col] = X_val[col].fillna(mode_val)\n",
    "            X_test[col] = X_test[col].fillna(mode_val)\n",
    "            encoders[f'{col}_mode'] = mode_val\n",
    "    \n",
    "    # Boolean: False\n",
    "    for col in X_train.select_dtypes(include=['bool']).columns:\n",
    "        if X_train[col].isnull().sum() > 0:\n",
    "            X_train[col] = X_train[col].fillna(False)\n",
    "            X_val[col] = X_val[col].fillna(False)\n",
    "            X_test[col] = X_test[col].fillna(False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"  âœ“ Domain knowledge fills completed\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 6. FEATURE ENGINEERING\n",
    "    # ========================================================================\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"6. FEATURE ENGINEERING\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    reference_date = pd.Timestamp('2018-12-06')\n",
    "    \n",
    "    # Date features - Process each dataset directly (FIXED: proper variable reference)\n",
    "    for df in [X_train, X_val, X_test]:\n",
    "        if 'host_since' in df.columns:\n",
    "            df['host_tenure_days'] = (reference_date - df['host_since']).dt.days\n",
    "            df['host_tenure_years'] = df['host_tenure_days'] / 365.25\n",
    "            df['host_since_year'] = df['host_since'].dt.year\n",
    "            df['host_since_month'] = df['host_since'].dt.month\n",
    "            df['host_since_dayofweek'] = df['host_since'].dt.dayofweek\n",
    "            df['host_since_month_sin'] = np.sin(2 * np.pi * df['host_since_month'] / 12)\n",
    "            df['host_since_month_cos'] = np.cos(2 * np.pi * df['host_since_month'] / 12)\n",
    "        \n",
    "        if 'first_review' in df.columns:\n",
    "            df['days_since_first_review'] = (reference_date - df['first_review']).dt.days\n",
    "            df['days_since_first_review'] = df['days_since_first_review'].fillna(0)\n",
    "        \n",
    "        if 'last_review' in df.columns:\n",
    "            df['days_since_last_review'] = (reference_date - df['last_review']).dt.days\n",
    "            df['days_since_last_review'] = df['days_since_last_review'].fillna(9999)\n",
    "        \n",
    "        if 'first_review' in df.columns and 'last_review' in df.columns:\n",
    "            df['review_period_days'] = (df['last_review'] - df['first_review']).dt.days\n",
    "            df['review_period_days'] = df['review_period_days'].fillna(0)\n",
    "        \n",
    "        # Text features\n",
    "        text_columns = ['name', 'summary', 'space', 'description', 'neighborhood_overview',\n",
    "                        'notes', 'transit', 'access', 'interaction', 'house_rules']\n",
    "        for col in text_columns:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_length'] = df[col].astype(str).str.len()\n",
    "                df[f'{col}_word_count'] = df[col].astype(str).str.split().str.len()\n",
    "        \n",
    "        # Amenities features\n",
    "        if 'amenities' in df.columns:\n",
    "            df['amenities_count'] = df['amenities'].astype(str).str.count(',') + 1\n",
    "            df['amenities_count'] = df['amenities_count'].replace({1: 0})\n",
    "            df['has_wifi'] = df['amenities'].str.contains('wifi|internet', case=False, na=False).astype(int)\n",
    "            df['has_kitchen'] = df['amenities'].str.contains('kitchen', case=False, na=False).astype(int)\n",
    "            df['has_tv'] = df['amenities'].str.contains('tv', case=False, na=False).astype(int)\n",
    "            df['has_parking'] = df['amenities'].str.contains('parking', case=False, na=False).astype(int)\n",
    "            df['has_ac'] = df['amenities'].str.contains('air conditioning|ac', case=False, na=False).astype(int)\n",
    "            df['has_heating'] = df['amenities'].str.contains('heating', case=False, na=False).astype(int)\n",
    "        \n",
    "        # Host verifications\n",
    "        if 'host_verifications' in df.columns:\n",
    "            df['host_verifications_count'] = df['host_verifications'].astype(str).str.count(',') + 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"  âœ“ Feature engineering completed\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 7. CLEANUP - DROP ORIGINAL TEXT/DATE COLUMNS\n",
    "    # ========================================================================\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"7. CLEANUP\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Drop original date columns\n",
    "    date_cols_to_drop = ['host_since', 'first_review', 'last_review']\n",
    "    X_train = X_train.drop(columns=[c for c in date_cols_to_drop if c in X_train.columns])\n",
    "    X_val = X_val.drop(columns=[c for c in date_cols_to_drop if c in X_val.columns])\n",
    "    X_test = X_test.drop(columns=[c for c in date_cols_to_drop if c in X_test.columns])\n",
    "    \n",
    "    # Drop original text columns\n",
    "    text_cols_to_drop = ['name', 'summary', 'space', 'description', 'neighborhood_overview',\n",
    "                         'notes', 'transit', 'access', 'interaction', 'house_rules',\n",
    "                         'amenities', 'host_verifications', 'host_about']\n",
    "    X_train = X_train.drop(columns=[c for c in text_cols_to_drop if c in X_train.columns], errors='ignore')\n",
    "    X_val = X_val.drop(columns=[c for c in text_cols_to_drop if c in X_val.columns], errors='ignore')\n",
    "    X_test = X_test.drop(columns=[c for c in text_cols_to_drop if c in X_test.columns], errors='ignore')\n",
    "    \n",
    "    # Drop other irrelevant columns\n",
    "    other_drops = ['street', 'city', 'state', 'zipcode', 'market', 'smart_location',\n",
    "                   'country', 'country_code', 'calendar_updated']\n",
    "    X_train = X_train.drop(columns=[c for c in other_drops if c in X_train.columns], errors='ignore')\n",
    "    X_val = X_val.drop(columns=[c for c in other_drops if c in X_val.columns], errors='ignore')\n",
    "    X_test = X_test.drop(columns=[c for c in other_drops if c in X_test.columns], errors='ignore')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  âœ“ Dropped original text/date columns\")\n",
    "        print(f\"  Shape: Train {X_train.shape}, Val {X_val.shape}, Test {X_test.shape}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 8. CATEGORICAL ENCODING\n",
    "    # ========================================================================\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"8. CATEGORICAL ENCODING\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Identify categorical features\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Separate by cardinality\n",
    "    low_cardinality = []\n",
    "    target_encode_cols = []\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        n_unique = X_train[col].nunique()\n",
    "        if n_unique < 10:\n",
    "            low_cardinality.append(col)\n",
    "        else:\n",
    "            target_encode_cols.append(col)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Low cardinality (<10 unique): {len(low_cardinality)} features\")\n",
    "        print(f\"  High cardinality (>=10 unique): {len(target_encode_cols)} features (will target encode)\")\n",
    "    \n",
    "    # One-hot encode low cardinality\n",
    "    if low_cardinality:\n",
    "        X_train = pd.get_dummies(X_train, columns=low_cardinality, prefix=low_cardinality,\n",
    "                                drop_first=True, dtype=int)\n",
    "        X_val = pd.get_dummies(X_val, columns=low_cardinality, prefix=low_cardinality,\n",
    "                              drop_first=True, dtype=int)\n",
    "        X_test = pd.get_dummies(X_test, columns=low_cardinality, prefix=low_cardinality,\n",
    "                               drop_first=True, dtype=int)\n",
    "        \n",
    "        # Align columns\n",
    "        train_cols = set(X_train.columns)\n",
    "        val_cols = set(X_val.columns)\n",
    "        test_cols = set(X_test.columns)\n",
    "        all_cols = train_cols.union(val_cols).union(test_cols)\n",
    "        \n",
    "        for col in all_cols:\n",
    "            if col not in X_train.columns:\n",
    "                X_train[col] = 0\n",
    "            if col not in X_val.columns:\n",
    "                X_val[col] = 0\n",
    "            if col not in X_test.columns:\n",
    "                X_test[col] = 0\n",
    "        \n",
    "        # Ensure same column order\n",
    "        X_val = X_val[X_train.columns]\n",
    "        X_test = X_test[X_train.columns]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  âœ“ One-hot encoded {len(low_cardinality)} features\")\n",
    "    \n",
    "    # Target encode high cardinality (FIT ON TRAIN ONLY!)\n",
    "    if target_encode_cols:\n",
    "        if verbose:\n",
    "            print(f\"  Target encoding {len(target_encode_cols)} high-cardinality features...\")\n",
    "        \n",
    "        for col in target_encode_cols:\n",
    "            if col in X_train.columns:\n",
    "                # Calculate means from TRAINING DATA ONLY\n",
    "                train_with_target = X_train[[col]].copy()\n",
    "                train_with_target['price'] = y_train.values\n",
    "                target_means = train_with_target.groupby(col)['price'].mean()\n",
    "                global_mean = y_train.mean()\n",
    "                \n",
    "                # Store encoder\n",
    "                encoders[f'{col}_target_encoder'] = {'means': target_means, 'global_mean': global_mean}\n",
    "                \n",
    "                # Apply to all sets\n",
    "                X_train[f'{col}_target_encoded'] = X_train[col].map(target_means).fillna(global_mean)\n",
    "                X_val[f'{col}_target_encoded'] = X_val[col].map(target_means).fillna(global_mean)\n",
    "                X_test[f'{col}_target_encoded'] = X_test[col].map(target_means).fillna(global_mean)\n",
    "        \n",
    "        # Drop original columns\n",
    "        X_train = X_train.drop(columns=target_encode_cols)\n",
    "        X_val = X_val.drop(columns=target_encode_cols)\n",
    "        X_test = X_test.drop(columns=target_encode_cols)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  âœ“ Target encoding completed\")\n",
    "    \n",
    "    # Convert booleans to int\n",
    "    for col in X_train.select_dtypes(include=['bool']).columns:\n",
    "        X_train[col] = X_train[col].astype(int)\n",
    "        X_val[col] = X_val[col].astype(int)\n",
    "        X_test[col] = X_test[col].astype(int)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Final shape after encoding: Train {X_train.shape}, Val {X_val.shape}, Test {X_test.shape}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 9. HANDLE REMAINING MISSING VALUES (Based on TRAINING data)\n",
    "    # ========================================================================\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"9. HANDLE REMAINING MISSING VALUES\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"  NaN count BEFORE: Train={X_train.isnull().sum().sum()}, \" +\n",
    "              f\"Val={X_val.isnull().sum().sum()}, Test={X_test.isnull().sum().sum()}\")\n",
    "    \n",
    "    if X_train.isnull().sum().sum() > 0:\n",
    "        # Impute with TRAINING median\n",
    "        numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if X_train[col].isnull().sum() > 0:\n",
    "                train_median = X_train[col].median()\n",
    "                encoders[f'{col}_median'] = train_median\n",
    "                \n",
    "                X_train[col] = X_train[col].fillna(train_median)\n",
    "                X_val[col] = X_val[col].fillna(train_median)\n",
    "                X_test[col] = X_test[col].fillna(train_median)\n",
    "    \n",
    "    # Final safety check\n",
    "    if X_train.isnull().sum().sum() > 0 or X_val.isnull().sum().sum() > 0 or X_test.isnull().sum().sum() > 0:\n",
    "        X_train = X_train.fillna(0)\n",
    "        X_val = X_val.fillna(0)\n",
    "        X_test = X_test.fillna(0)\n",
    "        if verbose:\n",
    "            print(\"  âœ“ Filled remaining NaN with 0\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  NaN count AFTER: Train={X_train.isnull().sum().sum()}, \" +\n",
    "              f\"Val={X_val.isnull().sum().sum()}, Test={X_test.isnull().sum().sum()}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 10. OUTLIER TREATMENT (Based on TRAINING data)\n",
    "    # ========================================================================\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"10. OUTLIER TREATMENT\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Domain-based caps\n",
    "    if 'minimum_nights' in X_train.columns:\n",
    "        X_train['minimum_nights'] = X_train['minimum_nights'].clip(upper=365)\n",
    "        X_val['minimum_nights'] = X_val['minimum_nights'].clip(upper=365)\n",
    "        X_test['minimum_nights'] = X_test['minimum_nights'].clip(upper=365)\n",
    "        if verbose:\n",
    "            print(\"  âœ“ Capped minimum_nights at 365\")\n",
    "    \n",
    "    if 'maximum_nights' in X_train.columns:\n",
    "        X_train['maximum_nights'] = X_train['maximum_nights'].clip(upper=730)\n",
    "        X_val['maximum_nights'] = X_val['maximum_nights'].clip(upper=730)\n",
    "        X_test['maximum_nights'] = X_test['maximum_nights'].clip(upper=730)\n",
    "        if verbose:\n",
    "            print(\"  âœ“ Capped maximum_nights at 730\")\n",
    "    \n",
    "    if 'accommodates' in X_train.columns:\n",
    "        X_train['accommodates'] = X_train['accommodates'].clip(upper=16)\n",
    "        X_val['accommodates'] = X_val['accommodates'].clip(upper=16)\n",
    "        X_test['accommodates'] = X_test['accommodates'].clip(upper=16)\n",
    "        if verbose:\n",
    "            print(\"  âœ“ Capped accommodates at 16\")\n",
    "    \n",
    "    # Winsorize based on TRAINING quantiles\n",
    "    winsorize_cols = ['cleaning_fee', 'security_deposit']\n",
    "    for col in winsorize_cols:\n",
    "        if col in X_train.columns:\n",
    "            cap_val = X_train[col].quantile(0.99)\n",
    "            encoders[f'{col}_99th'] = cap_val\n",
    "            \n",
    "            X_train[col] = X_train[col].clip(upper=cap_val)\n",
    "            X_val[col] = X_val[col].clip(upper=cap_val)\n",
    "            X_test[col] = X_test[col].clip(upper=cap_val)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  âœ“ Winsorized {col} at 99th percentile: {cap_val:.2f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"âœ… PREPROCESSING COMPLETE!\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"  Final shapes:\")\n",
    "        print(f\"    Train: {X_train.shape}\")\n",
    "        print(f\"    Val:   {X_val.shape}\")\n",
    "        print(f\"    Test:  {X_test.shape}\")\n",
    "    \n",
    "    feature_names = X_train.columns.tolist()\n",
    "    \n",
    "    return X_train, X_val, X_test, feature_names, encoders\n",
    "\n",
    "\n",
    "print(\"âœ“ Preprocessing function defined successfully\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: DEFINE PREPROCESSING FUNCTIONS\n",
      "================================================================================\n",
      "âœ“ Preprocessing function defined successfully\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "ffc3cf31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T19:45:23.011976Z",
     "start_time": "2025-11-13T19:45:21.849080Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "STEP 5: APPLY PREPROCESSING TO SPLIT DATA\n",
    "==========================================\n",
    "Apply the preprocessing function to train, val, and test sets\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: APPLY PREPROCESSING TO SPLIT DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_clean, X_val_clean, X_test_clean, feature_names, encoders = preprocess_data(\n",
    "    X_train, X_val, X_test, y_train, verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… PREPROCESSING APPLIED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nFinal Dataset Summary:\")\n",
    "print(f\"  Train:      {X_train_clean.shape[0]:,} samples Ã— {X_train_clean.shape[1]} features\")\n",
    "print(f\"  Validation: {X_val_clean.shape[0]:,} samples Ã— {X_val_clean.shape[1]} features\")\n",
    "print(f\"  Test:       {X_test_clean.shape[0]:,} samples Ã— {X_test_clean.shape[1]} features\")\n",
    "print(f\"\\n  Total features: {len(feature_names)}\")\n",
    "print(f\"  Encoders/transformers stored: {len(encoders)}\")\n",
    "\n",
    "# Display sample of calendar features\n",
    "calendar_features = [col for col in feature_names if 'calendar' in col.lower() or 'availability' in col.lower()]\n",
    "if calendar_features:\n",
    "    print(f\"\\nðŸ“… Calendar-derived features ({len(calendar_features)}):\")\n",
    "    for feat in calendar_features:\n",
    "        print(f\"  - {feat}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: APPLY PREPROCESSING TO SPLIT DATA\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1. DROPPING IRRELEVANT COLUMNS\n",
      "--------------------------------------------------------------------------------\n",
      "  âœ“ Dropped 22 columns\n",
      "  Shape: Train (12016, 83), Val (4006, 83), Test (4006, 83)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2. TYPE CONVERSION\n",
      "--------------------------------------------------------------------------------\n",
      "  âœ“ Type conversion completed\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3. LOGIC ERROR FIXING\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4. DROP HIGH MISSING COLUMNS\n",
      "--------------------------------------------------------------------------------\n",
      "  âœ“ Dropped 3 columns with >70% missing in training data\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5. DOMAIN KNOWLEDGE FILLS\n",
      "--------------------------------------------------------------------------------\n",
      "  âœ“ Filled security_deposit with 0\n",
      "  âœ“ Filled host_neighbourhood from neighbourhood_cleansed\n",
      "  âœ“ Filled 8 review columns with 0\n",
      "  âœ“ Domain knowledge fills completed\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6. FEATURE ENGINEERING\n",
      "--------------------------------------------------------------------------------\n",
      "  âœ“ Feature engineering completed\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "7. CLEANUP\n",
      "--------------------------------------------------------------------------------\n",
      "  âœ“ Dropped original text/date columns\n",
      "  Shape: Train (12016, 93), Val (4006, 93), Test (4006, 93)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "8. CATEGORICAL ENCODING\n",
      "--------------------------------------------------------------------------------\n",
      "  Low cardinality (<10 unique): 7 features\n",
      "  High cardinality (>=10 unique): 6 features (will target encode)\n",
      "  âœ“ One-hot encoded 7 features\n",
      "  Target encoding 6 high-cardinality features...\n",
      "  âœ“ Target encoding completed\n",
      "  Final shape after encoding: Train (12016, 102), Val (4006, 102), Test (4006, 102)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9. HANDLE REMAINING MISSING VALUES\n",
      "--------------------------------------------------------------------------------\n",
      "  NaN count BEFORE: Train=24615, Val=8260, Test=8329\n",
      "  NaN count AFTER: Train=0, Val=0, Test=0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10. OUTLIER TREATMENT\n",
      "--------------------------------------------------------------------------------\n",
      "  âœ“ Capped minimum_nights at 365\n",
      "  âœ“ Capped maximum_nights at 730\n",
      "  âœ“ Capped accommodates at 16\n",
      "  âœ“ Winsorized cleaning_fee at 99th percentile: 120.00\n",
      "  âœ“ Winsorized security_deposit at 99th percentile: 1000.00\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "âœ… PREPROCESSING COMPLETE!\n",
      "--------------------------------------------------------------------------------\n",
      "  Final shapes:\n",
      "    Train: (12016, 102)\n",
      "    Val:   (4006, 102)\n",
      "    Test:  (4006, 102)\n",
      "\n",
      "================================================================================\n",
      "âœ… PREPROCESSING APPLIED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "Final Dataset Summary:\n",
      "  Train:      12,016 samples Ã— 102 features\n",
      "  Validation: 4,006 samples Ã— 102 features\n",
      "  Test:       4,006 samples Ã— 102 features\n",
      "\n",
      "  Total features: 102\n",
      "  Encoders/transformers stored: 36\n",
      "\n",
      "ðŸ“… Calendar-derived features (11):\n",
      "  - has_availability\n",
      "  - availability_30\n",
      "  - availability_60\n",
      "  - availability_90\n",
      "  - availability_365\n",
      "  - avg_calendar_price\n",
      "  - min_calendar_price\n",
      "  - max_calendar_price\n",
      "  - availability_rate\n",
      "  - calendar_days_count\n",
      "  - calendar_available_days\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "36f101a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T19:45:24.700165Z",
     "start_time": "2025-11-13T19:45:24.586504Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "STEP 6: SCALING (FIT ON TRAIN, TRANSFORM ALL)\n",
    "==============================================\n",
    "Apply different scalers for model flexibility\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: SCALING (FIT ON TRAIN, TRANSFORM ALL)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# StandardScaler (best for most algorithms)\n",
    "scaler_standard = StandardScaler()\n",
    "X_train_standard = scaler_standard.fit_transform(X_train_clean)\n",
    "X_val_standard = scaler_standard.transform(X_val_clean)\n",
    "X_test_standard = scaler_standard.transform(X_test_clean)\n",
    "\n",
    "print(f\"\\nâœ“ StandardScaler applied:\")\n",
    "print(f\"  Train - mean: {X_train_standard.mean():.6f}, std: {X_train_standard.std():.6f}\")\n",
    "print(f\"  Val   - mean: {X_val_standard.mean():.6f}, std: {X_val_standard.std():.6f}\")\n",
    "print(f\"  Test  - mean: {X_test_standard.mean():.6f}, std: {X_test_standard.std():.6f}\")\n",
    "\n",
    "# MinMaxScaler (for neural networks)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_minmax = scaler_minmax.fit_transform(X_train_clean)\n",
    "X_val_minmax = scaler_minmax.transform(X_val_clean)\n",
    "X_test_minmax = scaler_minmax.transform(X_test_clean)\n",
    "\n",
    "print(f\"\\nâœ“ MinMaxScaler applied:\")\n",
    "print(f\"  Train - range: [{X_train_minmax.min():.6f}, {X_train_minmax.max():.6f}]\")\n",
    "print(f\"  Val   - range: [{X_val_minmax.min():.6f}, {X_val_minmax.max():.6f}]\")\n",
    "print(f\"  Test  - range: [{X_test_minmax.min():.6f}, {X_test_minmax.max():.6f}]\")\n",
    "\n",
    "# RobustScaler (for data with outliers)\n",
    "scaler_robust = RobustScaler()\n",
    "X_train_robust = scaler_robust.fit_transform(X_train_clean)\n",
    "X_val_robust = scaler_robust.transform(X_val_clean)\n",
    "X_test_robust = scaler_robust.transform(X_test_clean)\n",
    "\n",
    "print(f\"\\nâœ“ RobustScaler applied:\")\n",
    "print(f\"  Train - median: {np.median(X_train_robust):.6f}\")\n",
    "print(f\"  Val   - median: {np.median(X_val_robust):.6f}\")\n",
    "print(f\"  Test  - median: {np.median(X_test_robust):.6f}\")\n",
    "\n",
    "# Verify no NaN values after scaling\n",
    "print(f\"\\nâœ… NaN Check After Scaling:\")\n",
    "print(f\"  StandardScaler: Train={np.isnan(X_train_standard).sum()}, Val={np.isnan(X_val_standard).sum()}, Test={np.isnan(X_test_standard).sum()}\")\n",
    "print(f\"  MinMaxScaler:   Train={np.isnan(X_train_minmax).sum()}, Val={np.isnan(X_val_minmax).sum()}, Test={np.isnan(X_test_minmax).sum()}\")\n",
    "print(f\"  RobustScaler:   Train={np.isnan(X_train_robust).sum()}, Val={np.isnan(X_val_robust).sum()}, Test={np.isnan(X_test_robust).sum()}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: SCALING (FIT ON TRAIN, TRANSFORM ALL)\n",
      "================================================================================\n",
      "\n",
      "âœ“ StandardScaler applied:\n",
      "  Train - mean: 0.000000, std: 0.980196\n",
      "  Val   - mean: -0.002531, std: 0.967533\n",
      "  Test  - mean: 0.001433, std: 0.976170\n",
      "\n",
      "âœ“ MinMaxScaler applied:\n",
      "  Train - range: [0.000000, 1.000000]\n",
      "  Val   - range: [-0.014484, 1.005000]\n",
      "  Test  - range: [-0.037885, 1.105280]\n",
      "\n",
      "âœ“ RobustScaler applied:\n",
      "  Train - median: 0.000000\n",
      "  Val   - median: 0.000000\n",
      "  Test  - median: 0.000000\n",
      "\n",
      "âœ… NaN Check After Scaling:\n",
      "  StandardScaler: Train=0, Val=0, Test=0\n",
      "  MinMaxScaler:   Train=0, Val=0, Test=0\n",
      "  RobustScaler:   Train=0, Val=0, Test=0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "4d9005a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T19:45:29.739289Z",
     "start_time": "2025-11-13T19:45:29.242770Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "STEP 7: SAVE PROCESSED DATA\n",
    "============================\n",
    "Save all processed datasets for modeling\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 7: SAVE PROCESSED DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'processed_data'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"âœ“ Created directory: {output_dir}/\")\n",
    "\n",
    "# Save unscaled data (DataFrames) - FIXED: Reset index for both X and y\n",
    "train_unscaled = pd.concat([X_train_clean.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "val_unscaled = pd.concat([X_val_clean.reset_index(drop=True), y_val.reset_index(drop=True)], axis=1)\n",
    "test_unscaled = pd.concat([X_test_clean.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "train_unscaled.to_csv(f'{output_dir}/train_unscaled.csv', index=False)\n",
    "val_unscaled.to_csv(f'{output_dir}/val_unscaled.csv', index=False)\n",
    "test_unscaled.to_csv(f'{output_dir}/test_unscaled.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Saved unscaled data (CSV):\")\n",
    "print(f\"  - {output_dir}/train_unscaled.csv ({train_unscaled.shape})\")\n",
    "print(f\"  - {output_dir}/val_unscaled.csv ({val_unscaled.shape})\")\n",
    "print(f\"  - {output_dir}/test_unscaled.csv ({test_unscaled.shape})\")\n",
    "\n",
    "# Save scaled data (NumPy arrays) - StandardScaler\n",
    "np.save(f'{output_dir}/X_train_standard.npy', X_train_standard)\n",
    "np.save(f'{output_dir}/X_val_standard.npy', X_val_standard)\n",
    "np.save(f'{output_dir}/X_test_standard.npy', X_test_standard)\n",
    "np.save(f'{output_dir}/y_train.npy', y_train.values)\n",
    "np.save(f'{output_dir}/y_val.npy', y_val.values)\n",
    "np.save(f'{output_dir}/y_test.npy', y_test.values)\n",
    "\n",
    "print(f\"\\nâœ“ Saved scaled data - StandardScaler (NumPy):\")\n",
    "print(f\"  - {output_dir}/X_train_standard.npy\")\n",
    "print(f\"  - {output_dir}/X_val_standard.npy\")\n",
    "print(f\"  - {output_dir}/X_test_standard.npy\")\n",
    "print(f\"  - {output_dir}/y_train.npy, y_val.npy, y_test.npy\")\n",
    "\n",
    "# Save scaled data (NumPy arrays) - MinMaxScaler\n",
    "np.save(f'{output_dir}/X_train_minmax.npy', X_train_minmax)\n",
    "np.save(f'{output_dir}/X_val_minmax.npy', X_val_minmax)\n",
    "np.save(f'{output_dir}/X_test_minmax.npy', X_test_minmax)\n",
    "\n",
    "print(f\"\\nâœ“ Saved scaled data - MinMaxScaler (NumPy):\")\n",
    "print(f\"  - {output_dir}/X_train_minmax.npy\")\n",
    "print(f\"  - {output_dir}/X_val_minmax.npy\")\n",
    "print(f\"  - {output_dir}/X_test_minmax.npy\")\n",
    "\n",
    "# Save scaled data (NumPy arrays) - RobustScaler\n",
    "np.save(f'{output_dir}/X_train_robust.npy', X_train_robust)\n",
    "np.save(f'{output_dir}/X_val_robust.npy', X_val_robust)\n",
    "np.save(f'{output_dir}/X_test_robust.npy', X_test_robust)\n",
    "\n",
    "print(f\"\\nâœ“ Saved scaled data - RobustScaler (NumPy):\")\n",
    "print(f\"  - {output_dir}/X_train_robust.npy\")\n",
    "print(f\"  - {output_dir}/X_val_robust.npy\")\n",
    "print(f\"  - {output_dir}/X_test_robust.npy\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names_df = pd.DataFrame({'feature': feature_names})\n",
    "feature_names_df.to_csv(f'{output_dir}/feature_names.csv', index=False)\n",
    "print(f\"\\nâœ“ Saved feature names:\")\n",
    "print(f\"  - {output_dir}/feature_names.csv ({len(feature_names)} features)\")\n",
    "\n",
    "# Save scalers using pickle\n",
    "import pickle\n",
    "\n",
    "with open(f'{output_dir}/scaler_standard.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_standard, f)\n",
    "\n",
    "with open(f'{output_dir}/scaler_minmax.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_minmax, f)\n",
    "\n",
    "with open(f'{output_dir}/scaler_robust.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_robust, f)\n",
    "\n",
    "with open(f'{output_dir}/encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(encoders, f)\n",
    "\n",
    "print(f\"\\nâœ“ Saved scalers and encoders (pickle):\")\n",
    "print(f\"  - {output_dir}/scaler_standard.pkl\")\n",
    "print(f\"  - {output_dir}/scaler_minmax.pkl\")\n",
    "print(f\"  - {output_dir}/scaler_robust.pkl\")\n",
    "print(f\"  - {output_dir}/encoders.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… ALL DATA SAVED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 7: SAVE PROCESSED DATA\n",
      "================================================================================\n",
      "âœ“ Created directory: processed_data/\n",
      "\n",
      "âœ“ Saved unscaled data (CSV):\n",
      "  - processed_data/train_unscaled.csv ((12016, 103))\n",
      "  - processed_data/val_unscaled.csv ((4006, 103))\n",
      "  - processed_data/test_unscaled.csv ((4006, 103))\n",
      "\n",
      "âœ“ Saved scaled data - StandardScaler (NumPy):\n",
      "  - processed_data/X_train_standard.npy\n",
      "  - processed_data/X_val_standard.npy\n",
      "  - processed_data/X_test_standard.npy\n",
      "  - processed_data/y_train.npy, y_val.npy, y_test.npy\n",
      "\n",
      "âœ“ Saved scaled data - MinMaxScaler (NumPy):\n",
      "  - processed_data/X_train_minmax.npy\n",
      "  - processed_data/X_val_minmax.npy\n",
      "  - processed_data/X_test_minmax.npy\n",
      "\n",
      "âœ“ Saved scaled data - RobustScaler (NumPy):\n",
      "  - processed_data/X_train_robust.npy\n",
      "  - processed_data/X_val_robust.npy\n",
      "  - processed_data/X_test_robust.npy\n",
      "\n",
      "âœ“ Saved feature names:\n",
      "  - processed_data/feature_names.csv (102 features)\n",
      "\n",
      "âœ“ Saved scalers and encoders (pickle):\n",
      "  - processed_data/scaler_standard.pkl\n",
      "  - processed_data/scaler_minmax.pkl\n",
      "  - processed_data/scaler_robust.pkl\n",
      "  - processed_data/encoders.pkl\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL DATA SAVED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a871954d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T19:45:41.726692Z",
     "start_time": "2025-11-13T19:45:41.720659Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "FINAL SUMMARY\n",
    "=============\n",
    "Complete overview of the data cleaning pipeline\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸŽ‰ DATA CLEANING PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“Š PIPELINE SUMMARY:\")\n",
    "print(f\"  1. âœ“ Loaded listings_details.csv: {listings_df.shape}\")\n",
    "print(f\"  2. âœ“ Loaded calendar.csv: {calendar_df.shape}\")\n",
    "print(f\"  3. âœ“ Aggregated calendar features by listing_id\")\n",
    "print(f\"  4. âœ“ Merged with listings: {df.shape}\")\n",
    "print(f\"  5. âœ“ Split data (60/20/20):\")\n",
    "print(f\"       - Train: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"       - Val:   {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"       - Test:  {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"  6. âœ“ Applied preprocessing function to all splits\")\n",
    "print(f\"  7. âœ“ Applied 3 different scalers\")\n",
    "print(f\"  8. âœ“ Saved all processed data\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ FEATURE ENGINEERING:\")\n",
    "print(f\"  Original features: {X_train.shape[1]}\")\n",
    "print(f\"  Final features: {len(feature_names)}\")\n",
    "print(f\"  Calendar features: {len([f for f in feature_names if 'calendar' in f.lower() or 'availability' in f.lower()])}\")\n",
    "\n",
    "calendar_feats = [f for f in feature_names if 'calendar' in f.lower() or 'availability' in f.lower()]\n",
    "if calendar_feats:\n",
    "    print(f\"\\n  Calendar-derived features:\")\n",
    "    for feat in calendar_feats:\n",
    "        print(f\"    â€¢ {feat}\")\n",
    "\n",
    "print(\"\\nâœ… DATA QUALITY CHECKS:\")\n",
    "print(f\"  âœ“ No data leakage (all transformations fit on train only)\")\n",
    "print(f\"  âœ“ No NaN values in scaled data\")\n",
    "print(f\"  âœ“ Proper train/val/test isolation maintained\")\n",
    "print(f\"  âœ“ All splits have same features: {X_train_clean.shape[1]} columns\")\n",
    "\n",
    "print(\"\\nðŸ“ OUTPUT FILES:\")\n",
    "print(f\"  Directory: {output_dir}/\")\n",
    "print(f\"  â€¢ Unscaled data (CSV): train_unscaled.csv, val_unscaled.csv, test_unscaled.csv\")\n",
    "print(f\"  â€¢ Scaled data (NumPy):\")\n",
    "print(f\"    - StandardScaler: X_train_standard.npy, X_val_standard.npy, X_test_standard.npy\")\n",
    "print(f\"    - MinMaxScaler: X_train_minmax.npy, X_val_minmax.npy, X_test_minmax.npy\")\n",
    "print(f\"    - RobustScaler: X_train_robust.npy, X_val_robust.npy, X_test_robust.npy\")\n",
    "print(f\"  â€¢ Target arrays: y_train.npy, y_val.npy, y_test.npy\")\n",
    "print(f\"  â€¢ Metadata: feature_names.csv, encoders.pkl, scaler_*.pkl\")\n",
    "\n",
    "print(\"\\nðŸš€ READY FOR MODELING!\")\n",
    "print(\"  You can now load the processed data and train your models:\")\n",
    "print(\"  \")\n",
    "print(\"  # Load unscaled data\")\n",
    "print(\"  train_df = pd.read_csv('processed_data/train_unscaled.csv')\")\n",
    "print(\"  \")\n",
    "print(\"  # OR load scaled data\")\n",
    "print(\"  X_train = np.load('processed_data/X_train_standard.npy')\")\n",
    "print(\"  y_train = np.load('processed_data/y_train.npy')\")\n",
    "print(\"  \")\n",
    "print(\"=\" * 80)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸŽ‰ DATA CLEANING PIPELINE COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š PIPELINE SUMMARY:\n",
      "  1. âœ“ Loaded listings_details.csv: (20030, 96)\n",
      "  2. âœ“ Loaded calendar.csv: (7310950, 6)\n",
      "  3. âœ“ Aggregated calendar features by listing_id\n",
      "  4. âœ“ Merged with listings: (20028, 102)\n",
      "  5. âœ“ Split data (60/20/20):\n",
      "       - Train: 12,016 samples (60.0%)\n",
      "       - Val:   4,006 samples (20.0%)\n",
      "       - Test:  4,006 samples (20.0%)\n",
      "  6. âœ“ Applied preprocessing function to all splits\n",
      "  7. âœ“ Applied 3 different scalers\n",
      "  8. âœ“ Saved all processed data\n",
      "\n",
      "ðŸ“ˆ FEATURE ENGINEERING:\n",
      "  Original features: 101\n",
      "  Final features: 102\n",
      "  Calendar features: 11\n",
      "\n",
      "  Calendar-derived features:\n",
      "    â€¢ has_availability\n",
      "    â€¢ availability_30\n",
      "    â€¢ availability_60\n",
      "    â€¢ availability_90\n",
      "    â€¢ availability_365\n",
      "    â€¢ avg_calendar_price\n",
      "    â€¢ min_calendar_price\n",
      "    â€¢ max_calendar_price\n",
      "    â€¢ availability_rate\n",
      "    â€¢ calendar_days_count\n",
      "    â€¢ calendar_available_days\n",
      "\n",
      "âœ… DATA QUALITY CHECKS:\n",
      "  âœ“ No data leakage (all transformations fit on train only)\n",
      "  âœ“ No NaN values in scaled data\n",
      "  âœ“ Proper train/val/test isolation maintained\n",
      "  âœ“ All splits have same features: 102 columns\n",
      "\n",
      "ðŸ“ OUTPUT FILES:\n",
      "  Directory: processed_data/\n",
      "  â€¢ Unscaled data (CSV): train_unscaled.csv, val_unscaled.csv, test_unscaled.csv\n",
      "  â€¢ Scaled data (NumPy):\n",
      "    - StandardScaler: X_train_standard.npy, X_val_standard.npy, X_test_standard.npy\n",
      "    - MinMaxScaler: X_train_minmax.npy, X_val_minmax.npy, X_test_minmax.npy\n",
      "    - RobustScaler: X_train_robust.npy, X_val_robust.npy, X_test_robust.npy\n",
      "  â€¢ Target arrays: y_train.npy, y_val.npy, y_test.npy\n",
      "  â€¢ Metadata: feature_names.csv, encoders.pkl, scaler_*.pkl\n",
      "\n",
      "ðŸš€ READY FOR MODELING!\n",
      "  You can now load the processed data and train your models:\n",
      "  \n",
      "  # Load unscaled data\n",
      "  train_df = pd.read_csv('processed_data/train_unscaled.csv')\n",
      "  \n",
      "  # OR load scaled data\n",
      "  X_train = np.load('processed_data/X_train_standard.npy')\n",
      "  y_train = np.load('processed_data/y_train.npy')\n",
      "  \n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
